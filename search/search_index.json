{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"admins/admin_guide/","title":"Admin Guide","text":"<p>This guide is for administrators of the Open Front End. An administrator can deploy the Front End, manage the lifecycle of HPC clusters, set up networking and storage resources that support clusters, install applications, and manage user access.</p> <p>Basic administrator knowledge of the Google Cloud Plaform is needed in order to create projects and user accounts, but all other low-level administration tasks are handled by the Open Front End.</p> <p>Open Front End is a web application built upon the Django framework. By default, a single Django superuser is created at deployment time - this superuser then has administrator privileges. For large organisations that need additional administrators, additional Django superusers can be created from the Admin site within the Front End, once it has been deployed.</p> <p>The Open Front End uses the Google Cloud HPC Toolkit to provision resources for networks, filesystems and clusters using a service account that has its credentials registered to the Front End. The service account is also used for access management and billing.</p>"},{"location":"admins/admin_guide/#open-front-end-deployment","title":"Open Front End Deployment","text":"<p>The Open Front End is deployed from a client machine using a shell script. The script will guide the admin through the setup process, prompting for input of required parameters.</p> <p>The Front End can also be deployed using a configuration file specified with the <code>--config</code> option. In this case, the script will read the required parameters from the YAML file instead of prompting the user for input. This allows for an automated deployment process but it's important to ensure that all required parameters are specified properly in the configuration file before deploying.</p>"},{"location":"admins/admin_guide/#prerequisites","title":"Prerequisites","text":""},{"location":"admins/admin_guide/#client-machine","title":"Client Machine","text":"<p>The client machine must run Linux and there are a small number of prerequisites that must be installed before the Open Front End can be deployed:</p> <ul> <li>Git</li> <li>Terraform CLI installation</li> <li>Google Cloud CLI installation (<code>gcloud</code>   and <code>gsutil</code>)</li> </ul>"},{"location":"admins/admin_guide/#download-google-cloud-hpc-toolkit","title":"Download Google Cloud HPC Toolkit","text":"<p>The Google Cloud HPC Toolkit repository needs to be cloned to the client machine then the working directory needs to be chnaged to the Open Front End (<code>ofe</code>) directory:</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/hpc-toolkit.git\ncd hpc-toolkit/community/front-end/ofe\n</code></pre> <p>All further deployment actions must be performed from this directory.</p>"},{"location":"admins/admin_guide/#google-cloud-platform","title":"Google Cloud Platform","text":"<p>Your organisation must already have access to the Google Cloud Plaform (GCP) and be able to create projects and users. A project and a user account with enabled APIs and roles/permissions needs to be created. The user account must also be authenticated on the client machine to allow it to provision GCP resources.</p>"},{"location":"admins/admin_guide/#gcp-project","title":"GCP Project","text":"<p>A GCP project is required with the following APIs enabled:</p> <pre><code> Compute Engine API\n Cloud Monitoring API\n Cloud Logging API\n Cloud Pub/Sub API\n Cloud Resource Manager\n Identity and Access Management (IAM) API\n Cloud OS Login API\n Cloud Filestore API\n Cloud Billing API\n Vertex AI API\n</code></pre> <p>If these are not enabled, the deployment script will ask to enable them for you.</p>"},{"location":"admins/admin_guide/#gcp-user","title":"GCP User","text":"<p>A GCP user that is a member of the project will be able to deploy the Open  Front End, but must have appropriate privileges. A user that is an Owner of the project will automatically have all required roles and permissions.  Alternatively, another account with access to the project, but a limited set of roles can be used, which can help satisfy security concerns. The <code>gcloud</code> command can be used to switch to another account and apply IAM roles. IAM roles can also be applied via the GCP Console. The required roles are:</p> <pre><code> Compute Admin\n Storage Admin\n Pub/Sub Admin\n Create Service Accounts\n Delete Service Accounts\n Service Account User\n Project IAM Admin\n</code></pre> <p>If required, an even stricter, or least-privilege custom role can be created. Details are given in the Advanced Admin Guide.</p> <p>The user account must also be authenticated to deploy GCP resources, which can be done with the following command:</p> <pre><code>gcloud auth application-default login --project=&lt;PROJECT_ID&gt;\n</code></pre> <p>You will be prompted to open your web browser to authenticate. If further help is needed, please refer to the following GCP documentation:</p> <ul> <li>Creating and managing projects</li> <li>Enabling APIs</li> <li>Granting and roles/permissions</li> </ul>"},{"location":"admins/admin_guide/#deployment-process","title":"Deployment Process","text":""},{"location":"admins/admin_guide/#manual-deployment","title":"Manual Deployment","text":"<p>The Open Front End uses a deployment script run on the client machine and prompts the Admin for required parameters.</p> <ol> <li>Run <code>./deploy.sh</code></li> <li>Follow prompts to name the Open Front End VM instance, specify the GCP project,    zone and subnet (subnet is optional, and one will be created if required).    The hosting VM will be referred to as the service machine from now on.</li> <li>Follow prompts to enter domain name (DNS hostname) and IP address for the    service machine.</li> <li>For a production deployment, provide a domain name and static IP      address.  If a static IP is needed, follow the on-screen instructions. An      SSL certificate will automatically be obtained via      LetsEncrypt to secure the web application.</li> <li>For dev and testing purposes, the domain name and static IP address and      domain name can be left blank. The system can still be successfully      deployed and run with an ephemeral IP address, however OAuth2-based login      (see later) will not be available as this requires a publicly resolvable      domain name.</li> <li>Follow instructions to provide details for an Admin (the Django superuser)    account - username, password, email address.</li> <li>Confirm whether to create a service account for managing clusters.  This    will create a service account, and generate a credential, associated with    the GCP project specified earlier. This service account may not be needed if    using an existing one, or using multiple projects (see the    Advanced Admin Guide    for details).</li> <li>If selected, a credential file, <code>credential.json</code>. will be created in the      directory that needs to be registered in the Open Front End (see below).</li> <li>Confirm the parameters are correct when prompted and the deployment can    proceed.</li> <li>If confirmed, the VM instance will be created. It will take up to 15     minutes to provision the full service machine software stack.</li> <li>Once deployed, it will be possible to log into the Open Front End using the    Admin account details given, at the specified domain name or IP address    (output as <code>server_ip</code>).</li> </ol>"},{"location":"admins/admin_guide/#automated-deployment","title":"Automated Deployment","text":"<p>The Open Front End can also be deployed automatically using a YAML configuration file with all the required parameters. To deploy using this option, run <code>./deploy.sh --config &lt;path-to-config-file&gt;</code>. All required parameters for  deployment must be specified in the YAML file. If any required parameter is  missing or invalid, the script will exit with an error and no resources will be created or modified.</p> <p>If the DJANGO_SUPERUSER_PASSWORD environment variable is set, it will be used to set the Django superuser password instead of the value in the YAML file. This is a more secure way of handling passwords.</p> <p>To use a configuration file for automated deployment, follow these steps:</p> <ol> <li>Create a file called config.yaml:</li> </ol> <pre><code>deployment_name: MyDeployment\nproject_id: my-project-id\nzone: us-west1-a\nsubnet_name: my-subnet # (optional)\ndns_hostname: myhostname.com # (optional)\nip_address: 1.2.3.4 # (optional)\ndjango_superuser_username: sysadm\ndjango_superuser_password: Passw0rd! # (optional if DJANGO_SUPERUSER_PASSWORD is passed)\ndjango_superuser_email: sysadmin@example.com\n</code></pre> <ol> <li>Save the file in the same directory as the deploy.sh script.</li> <li>Open a terminal and navigate to the directory where the files are located.</li> <li>Run <code>./deploy.sh --config config.yaml</code></li> <li>The script will read the parameters from the config file and proceed with    the deployment process. If DJANGO_SUPERUSER_PASSWORD environment variable is    set, the script will use it to set the Django superuser password instead of    the value in the YAML file.</li> <li>The VM instance will now be created. It will take up to 15     minutes to provision the full service machine software stack.</li> <li>Once deployed, it will be possible to log into the Open Front End using the    Admin account details given, at the specified domain name or IP address    (output as <code>server_ip</code>).</li> </ol> <p>Important: To ensure that the Open Front End resources can be fully cleaned up at a later date, ensure that the directory containing the terraform configuration (<code>./tf</code>) is retained.</p>"},{"location":"admins/admin_guide/#post-deployment-credential-management","title":"Post-deployment Credential Management","text":"<p>To allow the Open Front End to manage cloud resources on behalf of users, a service account must be registered by the Admin. This is done by entering a credential corresponding to the service account - the credential will be a json file that needs to be copied into the Credentials form in the Open Front End.</p> <p>The deploy script will usually create a service account and credential (unless told not to), but other service accounts can be used (see the Advanced Admin Guide for details).</p>"},{"location":"admins/admin_guide/#network-management","title":"Network Management","text":"<p>All cloud systems begin with defining the network that components will exist in. Before a cluster or stand-alone filesystem can be created, the Admin must create the virtual private cloud (VPC) network. This is accomplished under the Networks tab. Note that network resources have their own life cycles and are managed independently.</p>"},{"location":"admins/admin_guide/#create-a-new-vpc","title":"Create a New VPC","text":"<p>To create a new network, the Admin must first select which credential should be used for this network, give the VPC a name then select the cloud region for the network.</p> <p>Upon clicking the Save button, the network is not immediately created. The Admin has to click Edit Subnet to create at least one subnet in the VPC. Once the network and subnet(s) are defined, click the Apply Cloud Changes button to trigger creation of the VPC and subnet(s).</p>"},{"location":"admins/admin_guide/#import-an-existing-vpc","title":"Import an Existing VPC","text":"<p>If your organisation already has predefined VPCs within the hosting GCP project, they can be imported. Simply select an existing VPC and associated subnets from the web interface to register them with the Open Front End. Imported VPCs can be used in exactly the same way as newly created ones.</p>"},{"location":"admins/admin_guide/#filesystem-management","title":"Filesystem Management","text":"<p>By default each cluster creates two shared filesystems: one at <code>/opt/cluster</code> to hold installed applications, and another at <code>/home</code> to hold job files for individual users. Both filesystems can be customised if required and additional ones can be created and mounted to the clusters. Note that filesystem resources have their own lifecycles and are managed independently, so they persist until explicitly deleted, and can be attached to several clusters in the same subnet.</p>"},{"location":"admins/admin_guide/#create-a-new-filesystem","title":"Create a New Filesystem","text":"<p>Currently, only GCP Filestore is supported. A GCP Filestore can be created from the Filesystems tab. A new Filestore has to be associated with an existing VPC and placed in a cloud zone. All performance tiers are supported.</p>"},{"location":"admins/admin_guide/#import-an-existing-filesystem","title":"Import an Existing Filesystem","text":"<p>Existing filesystems can be registered to this system and subsequently mounted by clusters. These can be existing NFS servers (like Filestore), or other filesystems which Linux has built-in mount support. For this to work, for each NFS server, provide an IP address and an export name. The IP address must be reachable by the VPC subnets intended to be used for clusters.</p> <p>An internal address can be used if the cluster shares the same VPC with the imported filesystem. Alternatively, system administrators can set up hybrid connectivity (such as extablishing network peering) beforing mounting the external filesystem located elsewhere on GCP.</p>"},{"location":"admins/admin_guide/#cluster-management","title":"Cluster Management","text":"<p>HPC clusters can be created after setting up the hosting VPC and any additional filesystems. The Open Front End can manage the whole lifecycle of clusters. Click the Clusters item in the main menu to list all existing clusters.</p>"},{"location":"admins/admin_guide/#cluster-status","title":"Cluster Status","text":"<p>Clusters can be in different states and their Actions menus adapt to this information to show different actions:</p> <ul> <li><code>n</code>: Cluster is being newly configured by user. At this stage, a new   cluster is being set up by an administrator. Only a database record exists,   and no cloud resource has been created yet. User is free to edit this cluster:   rename it, re-configure its associated network and storage components, and add   authorized users. Click Start from the cluster detail page to actually   provision the cluster on GCP.</li> <li><code>c</code>: Cluster is being created. This is a state when the backend   Terraform scripts is being invoked to commission the cloud resources for the   Cluster. This transient stage typically lasts for a few minutes.</li> <li><code>i</code>: Cluster is being initialised. This is a state when the cluster   hardware is already online, and Ansible playbooks are being executed to   install and configure the software environment of the Slurm controller and   login nodes. This transient stage can last for up to 15 minutes.</li> <li><code>r</code>: Cluster is ready for jobs. The cluster is now ready to use.   Applications can be installed and jobs can run on it. A Slurm job scheduler is   running on the controller node to orchestrate job activities.</li> <li><code>t</code>: Cluster is terminating. This is a transient state after Terraform   is being invoked to destroy the cluster. This stage can take a few minutes   when Terraform is working with the cloud platform to decommission cloud   resources.</li> <li><code>d</code>: Cluster has been destroyed. When destroyed, a cluster cannot be   brought back online. Only the relevant database record remains for information   archival purposes.</li> </ul> <p>A visual indication is shown on the website for the cluster being in creating, initialising or destroying states. Also, relevant web pages will refresh every 15 seconds to pick status changes.</p>"},{"location":"admins/admin_guide/#create-a-new-cluster","title":"Create a New Cluster","text":"<p>A typical workflow for creating a new cluster is as follows:</p> <ol> <li>At the bottom of the cluster list page, click the Add cluster button to    start creating a new cluster. In the next form, choose a cloud credential.    This is the Google Service Account which will create the cloud resources.    Click the Next button to go to a second form from which details of the    cluster can be specified.</li> <li>In the Create a new cluster form, give the new cluster a name. Cloud    resource names are subject to naming constraints and will be validated by the    system. In general, lower-case alpha-numeric names with hyphens are    accepted.</li> <li>From the Subnet dropdown list, select the subnet within which the cluster    resides.</li> <li>From the Cloud zone dropdown list, select a zone.</li> <li>From the Authorised users list, select users that are allowed to use this    cluster.</li> <li>Click the Save button to store the cluster settings in the database.    Continue from the Cluster Detail page.</li> <li>Click the Edit button to make additional changes. such as creating more    Slurm partitions for different compute node instance types, or mounting    additional filesystems.</li> <li>For filesystems, note the two existing shared filesystems defined by      default. Additional ones can be mounted if they have been created earlier.      Note the Mounting order parameter only matters if the Mount path      parameter has dependencies.</li> <li>For cluster partitions, note that one c2-standard-60 partition is      defined by default. Additional partitions can be added, supporting      different instance types. Enable or disable hyperthreading and node reuse      as appropriate. Also, placement group can be enabled (for C2 and C2D      partitions only). In the image field one can optionally supply a custom      image to be used by the compute nodes. Administrators should ensure such      an image is compatible to the CentOS 7 based machine image used by Slurm      GCP. Otherwise additional customisation done by this system might fail.</li> <li>Finally, save the configurations and click the Create button to trigger    the cluster creation.</li> </ol>"},{"location":"admins/admin_guide/#destroying-a-cluster","title":"Destroying a Cluster","text":"<p>To destroy a cluster, first find the list of clusters in the Clusters menu, then simply select Destroy in the Actions menu and confirm. Any jobs still running on the Cluster will be terminated.</p>"},{"location":"admins/admin_guide/#user-management","title":"User Management","text":""},{"location":"admins/admin_guide/#ssh-access-to-the-service-machine","title":"SSH Access to the Service Machine","text":"<p>SSH access to the service machine is possible for administration purposes. Administrators can choose from one of the following options:</p> <ul> <li>SSH directly from the GCP console.</li> <li>Add their public SSH key to the VM instance after deployment via GCP console.</li> <li>Add their SSH key to the GCP project to use on all VMs within the project.</li> </ul> <p>N.B The service machine is not, by default, configured to use the os-login service.</p>"},{"location":"admins/admin_guide/#setup-google-oauth2-login","title":"Setup Google OAuth2 Login","text":"<p>While it is possible to use a Django user account to access the Open Front End, and indeed doing so is required for some administration tasks, standard users must authenticate using their Google identities via Google OAuth2. This, combined with the use of Google OSLogin for access to clusters, ensures consistent Linux identities across VM instances that form the clusters. The Front End login is made possible by the django-allauth social login extension.</p> <p>For a working deployment, a fully-qualified domain name must be obtained and attached to the website as configured in the deployment script. Next, register the site with the hosting GCP project on the GCP console in the Credentials section under APIs and services category. Note that the Authorised JavaScript origins field should contain a callback URL in the following format: <code>https://&lt;domain_name&gt;/accounts/google/login/callback/</code></p> <p></p> <p>From the GCP console note the <code>client ID</code> and <code>client secret</code>, then return to the Admin site of the deployment and locate the social applications database table. A Google API record should have been created during the deployment. Replace the two placeholders with the <code>client ID</code> and <code>client secret</code>. The site is ready to accept Google login.</p> <p></p>"},{"location":"admins/admin_guide/#set-allowed-users-by-email-address","title":"Set Allowed Users by Email Address","text":"<p>Next, go to the Authorised user table. This is where further access control to the site is applied. Create new entries to grant access to users. A new entry can be:</p> <ul> <li>a valid domain name to grant access to multiple users from authorised organisations (e.g. @example.com)</li> <li>an email address to grant access to an individual user (e.g user.name@example.com)</li> </ul> <p>All login attempts that do not match these patterns will be rejected.</p>"},{"location":"admins/admin_guide/#note-on-external-users","title":"Note on External Users","text":"<p>If you wish to allow users from outside your Google Cloud organization to use the cluster you will need to additionally assign these users the <code>roles/compute.osLoginExternalUser</code> role at an Organization level (there is no way to assign at the project level).</p> <p>User accounts will be automatically created for users when they log into the FrontEnd for the first time, by default new accounts are created with quota disabled. To enable job submission for an account, administrators must enable compute quota from the Users page.</p>"},{"location":"admins/admin_guide/#user-compute-quota","title":"User Compute Quota","text":"<p>Currently three quota modes are supported:</p> <ul> <li>Quota Disabled (default): a User may not submit jobs.</li> <li>Unlimited quota: a User may submit an unlimited number of jobs.</li> <li>Limited quota: a User may submit jobs up to a total spend limit in USD.</li> </ul> <p>When limited quota is selected, an additional field quota amount will be available to set the total spend available to the user.</p>"},{"location":"admins/admin_guide/#application-management","title":"Application Management","text":"<p>Please see the Application Guide.</p>"},{"location":"admins/admin_guide/#workbench-management","title":"Workbench Management","text":"<p>Please see the Workbench Admin Guide.</p>"},{"location":"admins/admin_guide/#teardown-process","title":"Teardown Process","text":"<p>The HPC Toolkit repository contains a <code>teardown.sh</code> script that will destroy the running Open Front End instance. This script only removes the Open Front End, not resources started from it.</p> <p>Before running the teardown script, ensure that all clusters, VPCs, workbenches and filestores are removed using the Open Front End web interface before destroying it. These resources will otherwise persist and accrue costs.</p> <p>To teardown the Open Front End and its hosting infrastructure, run <code>./teardown.sh</code> on the original client machine within the same directory that was used to deploy the Open Front End.</p> <p>Note: If the <code>-y</code> flag is passed to the teardown script, all user questions will be answered with \"yes\". This is intended for automated deployments where user input is not possible.</p>"},{"location":"admins/admin_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"admins/admin_guide/#finding-log-files","title":"Finding Log Files","text":"<p>The service machine produces log files in <code>/opt/gcluster/run/</code>. These log files will show errors from the Django web application.</p> <p>Cloud resource deployment log files (from Terraform) are typically shown via the Open Front End. If those logs are not being shown, they can be found on the service machine under <code>/opt/gcluster/hpc-toolkit/frontend/(clusters|fs|vpc)/</code>. HPC Toolkit log files will also be found in those directories. The Terraform log files and status files will be down a few directories, based off of the Cluster Number, Deployment ID, and Terraform directory.</p> <p>On Cluster controllers, most of the useful log files for debugging can be retrieved by executing the Sync Cluster command. These include Slurm log files as well as general system log files. The daemon which communicates to the service machine logs to syslog, and can be viewed on the cluster controller node via <code>journalctl</code>, looking at the <code>ghpcfe_c2</code> service.</p> <p>Job logs and Spack application logs are uploaded upon job completion to Google Cloud Storage and viewable via the Front End.</p>"},{"location":"admins/admin_guide/#deployment-problems","title":"Deployment Problems","text":"<p>Most deployment problems are caused by not having the right permissions. If this is the case, an error message will normally show what permissions are missing. Use the IAM permissions reference to research this and identify additional roles to add to your user account.</p> <p>Before any attempt to redeploy the Open Front End, make sure to run <code>terraform destroy</code> in <code>hpc-toolkit/frontend/tf</code> to remove cloud resources that have been already created.</p>"},{"location":"admins/admin_guide/#cluster-problems","title":"Cluster Problems","text":"<p>The Open Front End should be quite reliable provisioning clusters. However, in cloud computing, errors occur from time to time, usually due to changes in back-end services. For example, a resource creation could fail because the hosting GCP project has ran out of certain resource quotas, or an upgrade of an underlying machine image might have introduced changes that are incompatible to the Open Front End, which then needs updating.</p> <p>It's not possible to capture all such situations. Here, a list of tips is given to help debug cluster creation problems. The Developer Guide contains a lot of details on how the back-end logic is handled, which can also help with certain issues.</p> <ul> <li>If a cluster is stuck at status c, something is wrong with the provisioning   of cluster hardware. SSH into the service machine and identify the directory   containing the run-time data for that cluster at    <code>frontend/clusters/cluster_&lt;cluster_id&gt;</code> where <code>&lt;cluster_id&gt;</code> can be found on   the Front End. Check the Terraform log files there for debugging information.</li> <li>If a cluster is stuck at status i, hardware resources should have been   commissioned properly and there is something wrong in the software   configuration stage. Locate the IP address of the Slurm controller node and   find its VM instance on GCP console. Check its related Serial port for the   system log. If necessary, SSH into the controller from the GCP console to   check Slurm logs under <code>/var/log/slurm/</code>.</li> </ul>"},{"location":"admins/admin_guide/#general-clean-up-tips","title":"General Clean-up Tips","text":"<ul> <li>If a cluster is stuck in i state, it is normally OK to find the Destroy   button from its Actions menu to destroy it.</li> <li>For failed network/filesystem/cluster creations, one may need to SSH into the   service machine, locate the run-time data directory, and manually run   <code>terraform destroy</code> there for clean up cloud resources.</li> <li>Certain database records might get corrupted and need to be removed for   failed clusters or network/filesystem components. This can be done from the   Django Admin site, although Admins need to exercise caution while   modifying the raw data in Django database.</li> </ul>"},{"location":"admins/advanced_admin_guide/","title":"Advanced Admin Guide","text":"<p>This guide outlines some advanced features for deploying and maintaining the Open Front End:</p> <ul> <li>Multi-project configuration</li> <li>Creation and management of service accounts</li> <li>Least-privilege roles for users</li> <li>Least-privilege enabled APIs for projects</li> </ul>"},{"location":"admins/advanced_admin_guide/#multi-project-configuration","title":"Multi-project Configuration","text":"<p>By default, the Open Front End will deploy using the specified project then it creates a service account which, when registered in the portal, allows the project to create HPC clusters and the associated resources. Usually, a single project for deploying then maintaining GCP resources is sufficient.</p> <p>It is possible though to separate concerns, so that one project deploys the Open Front End and another project provisions GCP resources via the Front End, allowing finer user access management. Separation of projects is a fairly straightforward matter of registering different service account credentials,  for the different projects, to the Open Front End. The process to create additional service accounts is outlined in the following section.</p>"},{"location":"admins/advanced_admin_guide/#service-account-management","title":"Service Account Management","text":"<p>Service accounts are used by the Front End to provision GCP resources on behalf of projects. These accounts are registered to the Front End using a generated json credential file. A default service account and credential is (optionally) created by the deployment script, however a more complex setup may be required for a multi-project configuration, or when a service account with custom roles is required.</p> <p>Service accounts can be created in a number of ways, as outlined below. In each case, the generated json credential is registered within the Open Front End in the same way, which is outlined in the Admin Guide.</p>"},{"location":"admins/advanced_admin_guide/#creating-a-service-account-via-the-helper-script","title":"Creating a Service Account via the Helper Script","text":"<p>The helper script included in the HPC Toolkit repository can be used to create a service account with the required basic roles/permissions when used by a user that has privileges within the project (e.g. Owner, or Editor). The roles/permissions could then be modified via <code>gcloud</code> or GCP Console (both covered below).</p> <p>To create a service account and credential file in json format:</p> <pre><code>script/service_account.sh create &lt;PROJECT_ID&gt; &lt;SERVICE_ACCOUNT_NAME&gt;\nscript/service_account.sh credential &lt;PROJECT_ID&gt; &lt;ACCOUNT_NAME&gt; &lt;PATH_TO_KEY_FILE&gt;\n</code></pre> <p>The script also has options to list, check and delete - see the built-in help for instructions:</p> <pre><code>script/service_account.sh help\n</code></pre> <p>Note to administrators/developers: if the roles required for a service account changes, the script must be modified (and docs including the list of roles below, updated).</p>"},{"location":"admins/advanced_admin_guide/#creating-a-service-account-via-the-gcp-console","title":"Creating a Service Account via the GCP Console","text":"<p>A user with project privileges can also create service accounts via the GCP Console:</p> <ol> <li>Log in to the GCP console and select    the GCP project that hosts the Open Front End.</li> <li>From the Navigation menu, select IAM &amp; Admin, then Service Accounts.    - Click the CREATE SERVICE ACCOUNT button.    - Name the service account, optionally provide a description, and then      click the CREATE button.</li> <li>Grant the service account the following roles (these are the same basic    roles the helper script would apply - for finer control see later section):    - Cloud Filestore Editor    - Compute Admin    - Create Service Accounts    - Delete Service Accounts    - Project IAM Admin    - Notebooks Admin    - Vertex AI administrator</li> <li>Click Done button.</li> <li>Locate the new service account from the list, click Manage Keys from the    Actions menu.    - Click ADD KEY, then Create new key.<ul> <li>Select JSON as key type, and click the CREATE button.</li> <li>A JSON key file will then be downloaded.</li> <li>Copy the generated JSON content which should then be pasted into the    credential creation form within the Open Front End.</li> </ul> </li> <li>Click Validate and Save to register the new credential to the Front End.</li> </ol>"},{"location":"admins/advanced_admin_guide/#creating-a-service-account-using-gcloud","title":"Creating a Service Account using <code>gcloud</code>","text":"<p>Alternatively the <code>gcloud</code> command line tool can be used to create a suitable service account (this is what the helper script does with additional checks). To create a service account with the basic required roles:</p> <pre><code>gcloud iam service-accounts create &lt;SERVICE_ACCOUNT_NAME&gt;\nfor roleid in file.editor \\\n              compute.admin \\\n              iam.serviceAccountCreator \\\n              iam.serviceAccountDelete \\\n              resourcemanager.projectIamAdmin \\\n              notebooks.admin aiplatform.admin; do \\\n     gcloud projects add-iam-policy-binding &lt;PROJECT_ID&gt; \\\n      --member=\"serviceAccount:&lt;SERVICE_ACCOUNT_NAME&gt;@&lt;PROJECT_ID&gt;.iam.gserviceaccount.com\" \\\n      --role=\"roles/$roleid\"; \\\ndone\n\ngcloud iam service-accounts keys create &lt;PATH_TO_KEY_FILE&gt; \\\n    --iam-account=&lt;SERVICE_ACCOUNT_NAME&gt;@&lt;PROJECT_ID&gt;.iam.gserviceaccount.com\n</code></pre> <p>Once complete, the service account key json text should be copied from <code>PATH_TO_KEY_FILE</code> into the credentials form on the Open Front End.</p> <p>The credential can now be used to create network, storage and compute resources from the Front End.</p> <p>The roles can be changed to give finer control as outlined in the next section.</p>"},{"location":"admins/advanced_admin_guide/#custom-rolespermissions-and-apis","title":"Custom Roles/Permissions and APIs","text":"<p>The projects and user account used for deploying the Open Front End can be more tightly controlled with respect to the enabled APIs and roles/permissions.</p>"},{"location":"admins/advanced_admin_guide/#user-account","title":"User Account","text":"<p>Rather than using Owner role, or the high-level roles stated in the Admin Guide, the user account deploying the Front End can use a custom set of least-privilege roles. The complete list of required permissions is as follows:</p> <pre><code> compute.acceleratorTypes.list\n compute.addresses.use\n compute.disks.create\n compute.disks.get\n compute.firewalls.create\n compute.firewalls.delete\n compute.firewalls.get\n compute.globalOperations.get\n compute.instances.create\n compute.instances.delete\n compute.instances.get\n compute.instances.getSerialPortOutput\n compute.instances.setLabels\n compute.instances.setMetadata\n compute.instances.setServiceAccount\n compute.instances.setTags\n compute.machineTypes.list\n compute.networks.create\n compute.networks.delete\n compute.networks.get\n compute.networks.updatePolicy\n compute.projects.get\n compute.regionOperations.get\n compute.routers.create\n compute.routers.delete\n compute.routers.get\n compute.routers.update\n compute.subnetworks.create\n compute.subnetworks.delete\n compute.subnetworks.get\n compute.subnetworks.use\n compute.subnetworks.useExternalIp\n compute.zoneOperations.get\n compute.zones.get\n compute.zones.list\n file.instances.create\n file.instances.delete\n file.instances.get\n file.operations.get\n iam.serviceAccounts.actAs\n iam.serviceAccounts.create\n iam.serviceAccounts.delete\n iam.serviceAccounts.get\n iam.serviceAccounts.getIamPolicy\n iam.serviceAccounts.setIamPolicy\n pubsub.subscriptions.create\n pubsub.subscriptions.delete\n pubsub.subscriptions.get\n pubsub.subscriptions.getIamPolicy\n pubsub.subscriptions.setIamPolicy\n pubsub.topics.attachSubscription\n pubsub.topics.create\n pubsub.topics.delete\n pubsub.topics.get\n pubsub.topics.getIamPolicy\n pubsub.topics.setIamPolicy\n resourcemanager.projects.get\n resourcemanager.projects.getIamPolicy\n resourcemanager.projects.setIamPolicy\n storage.buckets.create\n storage.buckets.delete\n storage.buckets.get\n storage.buckets.getIamPolicy\n storage.buckets.setIamPolicy\n storage.objects.create\n storage.objects.delete\n storage.objects.get\n storage.objects.list\n</code></pre>"},{"location":"admins/advanced_admin_guide/#project-apis","title":"Project APIs","text":"<p>In a multi-project configuration, the enabled project APIs can also be reduced to a subset of those APIs only needed for the functions required.</p> <p>If a project is only deploying the Open Front End, and not provisioning GCP resources via the Open Front End, only the following APIs need to be enabled:</p> <pre><code> Compute Engine API\n Cloud Monitoring API\n Cloud Logging API\n Cloud Pub/Sub API\n Cloud Resource Manager\n Identity and Access Management (IAM) API\n</code></pre> <p>A project that administers GCP resources via the Open Front End (that has a service account created within it, as covered above), needs the following APIs:</p> <pre><code> Compute Engine API\n Cloud Monitoring API\n Cloud Resource Manager\n Cloud Logging API\n Cloud OS Login API\n Cloud Filestore API\n Cloud Billing API\n Vertex AI API\n</code></pre>"},{"location":"admins/application_guide/","title":"Application Installation Guide","text":"<p>Administrators can install and manage applications in TKFE in the following ways:</p>"},{"location":"admins/application_guide/#install-spack-applications","title":"Install Spack applications","text":"<p>The recommended method of application installation is via Spack. Spack, an established package management system for HPC, contains build recipes of the most widely used open-source HPC applications. This method is completed automated. Spack installation is performed as a Slurm job. Simply choose a Slurm partition to run Spack.</p> <p>Advanced users may also customise the installation by specifying a Spack spec string.</p>"},{"location":"admins/application_guide/#install-custom-applications","title":"Install custom applications","text":"<p>For applications not yet covered by the Spack package repository, e.g., codes developed in-house, or those failed to build by Spack, use custom installations by specifying custom scripts containing steps to build the applications.</p>"},{"location":"admins/application_guide/#register-manually-installed-applications","title":"Register manually installed applications","text":"<p>Complex packages, such as some commercial applications that may require special steps to set up, can be installed manually on the cluster's shared filesystem. Once done, they can be registered with the FrontEnd so that future job submissions can be automated through the FrontEnd.</p>"},{"location":"admins/application_guide/#application-status","title":"Application status","text":"<p>Clicking the Applications item in the main menu leads to the application list page which displays all existing application installations. Applications can be in different states and their Actions menus adapt to this information to show different actions:</p> <ul> <li><code>n</code> - Application is being newly configured by an admin user through the web   interface. At this stage, only a database record exists in the system. The   user is free to edit this application, although in the case of a Spack   application  , most information is automatically populated. When ready,   clicking Spack Install from the Actions menu to initiate the installation   process.</li> <li><code>p</code> - Application is being prepared. In this state, application build is   triggered from the web interface and information is being passed to the   cluster.</li> <li><code>q</code> - In this state the Slurm job for building this application is queueing   on the target cluster. Note that all application installations are performed   on a compute node. This leaves the relatively lightweight controller and   login nodes to handle management tasks only, and also ensures the maximum   possible compatibility between the generated binary and the hardware to run   it in the future.</li> <li><code>i</code> - In this state, the Slurm job for building this application is running   on the target cluster. Spack is fully responsible for building this   application and managing its dependencies.</li> <li><code>r</code> - Spack build has completed successfully, and the application is ready    to run by authorised users on the target cluster.</li> <li><code>e</code> - Spack has somehow failed to build this application. Refer to the   debugging section of this document on how to debug a failed installation.</li> <li><code>x</code> - If a cluster has been destroyed, all applications on this cluster will   be marked in this status. Destroying a cluster won\u2019t affect the application   and job records stored in the database.</li> </ul> <p>A visual indication is shown on the website for any application installation in progress. Also, the relevant web pages will refresh every 15 seconds to pick status changes.</p>"},{"location":"admins/application_guide/#install-a-spack-application","title":"Install a Spack application","text":"<p>A typical workflow for installing a new Spack application is as follows:</p> <ol> <li>From the application list page, press the New application button. In the    next form, select the target cluster and choose Spack installation.</li> <li>In the Create a new Spack application form, type a keyword in the    Create a new Spack application form, and use the auto-completion function    to choose the Spack package to install. The Name and Version fields are    populated automatically. If Spack supports multiple versions of the    application, click the drop-down list there to select the desired version.</li> <li>Spack supports variants - applications built with customised compile-time    options. These may be special compiler flags or optional features that must    be switched on manually. Advanced users may supply additional specs using    the optional Spack spec field.    - For a guide to the Spack spec syntax see the Spack documentation    - By default, the GCC 11.2 compiler is used for building all applications.    - Other compilers may be specified with the <code>%</code> compiler specifier and an      optional version number using the <code>@</code> version specifier (e.g.,      <code>%intel@19.1.1.217</code>). Obviously, admin users are responsible for      installing and configuring those additional compilers and, if      applicable, arrange their licenses.    - Spack is configured in this system to use Intel MPI to build      application.    - Other MPI libraries may be specified with the ^ dependency specifier and      an optional version number.</li> <li>The Description field is populated automatically from the information found    in the Spack repository.</li> <li>Choose an Slurm partition from the drop-down list to run the Slurm job for    application installation. This, typically, should be the same partitions to    run the application in the future.</li> <li>Click the Save button. A database record is then created for this    application in the system. On the next page, click the Edit button to    modify the application settings; click the Delete button to delete this    record if desired; click the Spack install button to actually start    building this application on the cluster. The last step can take quite a    while to complete depending on the application. A visual indication is    given on the related web pages until the installation Slurm job is    completed.</li> <li>A successfully installed application will have its status updated to   \u2018ready\u2019. A New Job button becomes available from the Actions menu on the    application list page, or from the application detail page. The    User Guide contains additional information on how jobs can    be prepared and submitted.</li> </ol>"},{"location":"admins/application_guide/#application-problems","title":"Application problems","text":"<p>Spack installation is fairly reliable. However, there are thousands of packages in the Spack repository and packages are not always tested on all systems. If a Spack installation returns an error, first locate the Spack logs by clicking the View Logs button from the application detail page. Then identify from the Installation Error Log the root cause of the problem.</p> <p>Spack installation problems can happen with not only the package installed, but also its dependencies. There is no general way to debug Spack compilation problems. It may be helpful submit an interactive job to the cluster and debug Spack problems there manually. It is recommended to not build applications from the controller or login nodes, as the underlying processor may differ to the compute nodes.</p> <p>Complex bugs should be reported to Spack. If an easy fix can be found, note the procedure. This can be then used in a custom installation.</p>"},{"location":"admins/cluster_guide/","title":"Command and Control of Clusters","text":"<p>Previous incarnations of this Frontend relied on the frontend webserver instance being able to SSH directly to clusters in order to performance command and control (C2) operations. When clusters were created, an admin user was set that would accept a public ssh key for which the webserver owned the private key. This was largely straightfoward, and worked quite well. The clusters were also able to make HTTP API queries to the webserver.</p> <p>This works well in the case where webserver and clusters all have public IP addresses, and are able to receive inbound requests, but it breaks down in the case where a user may wish to have the compute clusters not be directly exposed to the public internet.</p> <p>The new approach is based around Google PubSub. The webserver and clusters now no longer directly communicate via SSH and HTTP, but rather send messages via Google Cloud. This offers the advantage of supporting clusters that are not publicly accessible, and removes the reliance on SSH connections between webserver and clusters.</p>"},{"location":"admins/cluster_guide/#pubsub-details","title":"PubSub details","text":"<p>During deployment of a new Frontend system, a new Google PubSub Topic will be created.  This centralized topic is used for all command and control traffice between the Frontend webserver and the client clusters. Individual Subscriptions are created for each cluster controller, as well as for the Frontend itself.  The Frontend creates a new subscription for each cluster, and informs the cluster that it is to use the newly-created subscription.</p> <p></p>"},{"location":"admins/cluster_guide/#message-delivery","title":"Message Delivery","text":"<p>Each subscription is filtered based off of a message attribute.  Messages to each Cluster MUST have an attribute <code>target=cluster_X</code> where <code>X</code> is the Cluster's unique ID. Messages to the Frontend MUST NOT have a <code>target</code> attribute.</p> <p>By using filtering in this way, and having a 1:1 mapping between Subscription and Recipient, we guarantee that the messages being sent are received by only the intended recipient.</p>"},{"location":"admins/cluster_guide/#message-schema","title":"Message Schema","text":"<p>Beyond the filtering attribute requirements previously discussed, the form of the messages are as follows:</p>"},{"location":"admins/cluster_guide/#common-attributes","title":"Common Attributes","text":"<ul> <li><code>command</code> - The command being sent as part of the message</li> <li><code>source</code> - The identity of the sender - corresponds to the <code>target</code> attribute. This is how the Frontend identifies which cluster sent the message</li> </ul>"},{"location":"admins/cluster_guide/#common-message-data","title":"Common Message Data","text":"<ul> <li><code>ackid</code> - A UUID generated by the system to identify a command/response pair.</li> </ul> <p>Each command will have additional data in the Message Data, specific to that command's requirements.</p>"},{"location":"admins/cluster_guide/#commands","title":"Commands","text":"<ul> <li><code>ACK</code> - Acknowledges a previous command, signals that the command is complete</li> <li><code>UPDATE</code> - Acknowledges a previous command, but signals that the command is not yet complete.  Can be sent in response to other commands multiple times, to be finally followed by an <code>ACK</code></li> <li><code>PING</code>, <code>PONG</code> - Testing commands.  Not typically used</li> <li><code>CLUSTER_STATUS</code> - Cluster command to Frontend to indicate a change in the status of the cluster. For example, to signal that the cluster has finished initialization and is ready for jobs.</li> <li><code>SYNC</code> - Command to cluster to synchronize with the Frontend, including updating Log Files, and potentially other activities in the future (such as setting user permissions).</li> <li><code>SPACK_INSTALL</code> - Install a Spack package</li> <li><code>RUN_JOB</code> - Submit a job on behalf of a user to SLURM</li> <li><code>REGISTER_USER_GCS</code> - Begin the process to register a user's GCS credentials with <code>gsutil</code>.</li> </ul>"},{"location":"admins/cluster_guide/#cluster-c2-daemon","title":"Cluster C2 Daemon","text":"<p>During startup of a cluster, a Daemon is installed which creates a Streaming Pull thread to Subscribe to the Cluster's Subscription.  This daemon is responsible for responding to C2 messages and following through on the message's requests, including submitting jobs to SLURM to install Spack packages, and run user's jobs.</p>"},{"location":"admins/cluster_guide/#security","title":"Security","text":"<p>The C2 topic is created at deployment time, as well as the subscription for the Frontend.  Topic creation permission is then no longer required by the Service Accounts of the Frontend or the Clusters.</p> <p>When a Cluster is created, a new Service Account is created for that Cluster.  This Service Account is then granted <code>pubsub.subscriber</code> permissions to the C2 Topic, and <code>pubsub.publisher</code> permissions to that cluster's own Subscription.  The Service Accounts for the clusters are created without any Google PubSub IAM permissions, so these policy bindings on the topic and subscription are the only PubSub IAM permissions granted to the cluster's service accounts.</p> <p>Sadly, the Frontend's Service Account must have either the role of <code>pubsub.admin</code> or a custom role, set at the project level.  This is because creating subscriptions and setting IAM Policy Bindings are actions done at the project level, rather than attached to the topic.</p> <p>By setting IAM policy bindings, we are able to grant permissions to service accounts which are associated with clusters which are in projects other than the base project where the Frontend resides.</p> <p>For example, if the Frontend is in GCP Project <code>Alpha</code>, the C2 Topic will also be in Project <code>Alpha</code>.  If a cluster is then created in project <code>Beta</code>, the Frontend will grant the cluster's Service Account IAM permissions within the <code>Alpha</code> project.</p>"},{"location":"admins/cluster_guide/#data-storage","title":"Data Storage","text":"<p>Clusters automatically upload job logs to a GCS bucket, which is specified at cluster creation time.  The Cluster's Service Account is granted ObjectAdmin permissions in order to create and update Log files in the GCS bucket.</p> <p>The Frontend webserver displays log files from the GCS bucket.</p>"},{"location":"admins/workbench_admin_guide/","title":"Workbench Management","text":"<p>The Workbench feature provides a way to create and control VertexAI Workbenches, which provide a single interactive development environment using a Jupyter Notebook perfect for pre/post processing of data. Workbenches can be located within the same VPC as other GCP resources managed by the FrontEnd.</p>"},{"location":"admins/workbench_admin_guide/#workbench-configuration","title":"Workbench Configuration","text":"<p>The first stage of configuration, after selecting the desired cloud credential, is to select the basic profile of the workbench including:</p> <ul> <li>Workbench Name</li> <li>Subnet</li> <li>Cloud Zone</li> <li>Trusted User</li> </ul> <p></p> <p>The subnet will define which regions the workbench can be located in. Workbenches are not available in all regions, see Workbench Documentation for more detail on currently available regions. Once a region is selected the cloud zone field will be populated with the available zones.</p>"},{"location":"admins/workbench_admin_guide/#trusted-users","title":"Trusted Users","text":"<p>The trusted user field will govern which user has access to the workbench. This is a 1:1 relationship as each workbench has a single instance owner that is set by the trusted user value. The workbench is then configured to run the jupyter notebook as the users OSLogin account. Access to the notebook is controlled by a proxy that requires the user to be logged into their google account to gain access.</p> <p>Workbench instances have a limited number of configurations: - Machine type - Boot disk type - Boot disk capacity - Image type</p> <p></p>"},{"location":"admins/workbench_admin_guide/#machine-type-workbench-presets","title":"Machine type &amp; Workbench Presets","text":"<p>An administrator can configure any type of machine type that is available. Users with the \"Normal User\" class will only be able to create workbenches using the preset machine type configurations while users with the \"Viewer\" class will not be able to create workbenches for themselves. The HPC toolkit frontend comes with some pre-configured workbench presets: - Small - 1x core with 3840 Memory (n1-standard-1) - Medium - 2x cores with 7680 Memory (n1-standard-2) - Large - 4x cores with 15360 Memory (n1-standard-4) - X-Large - 8x cores with 30720 Memory (n1-standard-8)</p> <p>Each of these have been created under the category \"Recommended\". Presets can be edited, deleted or new presets added via the admin panel where you can set the machine type and the category under which the user will see the preset.</p> <p></p>"},{"location":"admins/workbench_admin_guide/#workbench-storage","title":"Workbench Storage","text":"<p>The final setup of the workbench is to select any filesystems that are required to be mounted on the workbench. On this page the configuration fields will be disabled and no changes will be possible to the workbench configuration.</p> <p></p> <p>Within this configuration you can select from existing storage exports, the order they are mounted, and the mouth path in the filesystem. Storage will be mounted in the order according to the mount order which will be important if you are mounting storage within a sub-directory of another storage mount. Another important configuration to be aware of is that filesystems will only be mounted if the filestore or cluster is active and has an accurate IP address or hostname in the frontends database.</p>"},{"location":"admins/workbench_admin_guide/#workbench-problems","title":"Workbench problems","text":""},{"location":"admins/workbench_admin_guide/#storage-not-mounted","title":"Storage not mounted","text":"<p>If the expected filesystem storage has not been mounted or is not available the most likely cause is that the database does not have a hostname or IP address for the filestore or cluster targeted. An admin can resolve this by accessing the instance by SSH-ing into the GCP instance the runs the workbench and running <code>mount $IPADDRESS:/$TARGETDIR $MOUNTPOINT</code></p>"},{"location":"admins/workbench_admin_guide/#workbench-stuck-in-creating-status","title":"Workbench stuck in \"Creating\" status","text":"<p>If a workbench is stuck in \"Creating\" status this can be resolved by manually changing the status back to newly created in the admin portal and then starting the creation process again. Logs for this process can be seen at <code>$HPCtoolkitHome/frontend/workbenches/workbench_##/terraform/google/</code> where HPCtoolkitHome is normally /opt/gcluster and ## will be the id number of the workbench in question.</p>"},{"location":"developers/developer_guide/","title":"Developer Guide","text":""},{"location":"developers/developer_guide/#architecture-design","title":"Architecture design","text":"<p>The HPC Toolkit FrontEnd is a web application integrating several front-end and back-end technologies. Django, a high-level Python-based web framework, forms the foundation of the web application. The back-end business logics can mostly be delegated to Terraform to create GCP cloud infrastructure required by the HPC clusters. With HPC Toolkit, there is no need to define infrastructure configurations from scratch. Rather, a high-level description of the clusters are provided for it to generate Terraform configurations.</p> <p>The overall system design is described in the following figure:</p> <p> </p> <p>In most cases, end-users are expected to use and communicate with the cloud systems via the FrontEnd. Of course, users from a traditional supercomputing background may wish to work with HPC clusters from the command line. This is entirely possible through the Slurm login nodes.</p> <p>A single compute engine virtual machine, referred to as the service machine from now on, should be created to host the application server, webserver and database server. In large productions, these servers can of course be hosted on different machines if desired.</p> <p>The web application is built upon Django, a Python framework to develop data-driven dynamic websites. An nginx server is configured to serve the static files of the website, as well as proxying Django URLs to the application server. (Uvicorn is the chosen application server). Application data is stored in a file-based SQLite database which can be easily replaced by a managed SQL service for large production environments.</p> <p>From the web application, HPC clusters can be created on GCP by administrators. A typical HPC cluster contains a single Slurm controller node, and one or more login nodes, typically all running on low- to mid-range virtual machines. The controller node hosts the Slurm job scheduler. Through the job scheduler, compute nodes can be started or terminated as required. The controller node and login nodes all provide public IP addresses for administrators or users to SSH into, although doing so is not mandatory as day-to-day tasks can be performed via the web interface.</p> <p>The Slurm job scheduler supports partitions. Each partition can have compute nodes of different instance types. All major HPC capable instance types can be supported from a single cluster if so desired. Of course, it is also possible to create multiple clusters, which is entirely an operational decision by the administrators.</p> <p>For each cluster, two shared filesystems are created to host a system directory for applications and a home directory for users' job data. Additional filesystems can be created or imported, to be mounted to the clusters.</p> <p>For each deployment, a GCS bucket is created to hold supporting files, including configurations to build the service machine, and Ansible configurations to set up various Slurm nodes. The same GCS bucket is also served as a long-term backup to application and job data, including log files for most cloud operations and selected files created by jobs.</p> <p>Communication between the service machine and clusters is handled by Pub/Sub. For technical details, consult the Cluster Command &amp; Control document. Alternatively, there is an API layer around Django to allow incoming communication to the service machine.</p>"},{"location":"developers/developer_guide/#deploy-the-system","title":"Deploy the system","text":"<p>Please follow the deployment section in the Administrator\u2019s Guide to deploy the system for testing and development.</p> <p>Here are some notes from a developer's perspective:</p> <ul> <li>The deployment is done using Terraform.</li> <li>When <code>deploy.sh</code> is invoked, it validates the client machine's development   environment, collects configuration information from user, save input   variables in <code>tf/terraform.tfvars</code>, and invoke Terraform.</li> <li>The deploy script will check the GCP project being used has the correct     APIs enabled.  The list of required APIs is embedded in <code>deploy.sh</code>     script - this will need to be maintained.</li> <li>The deploy script will also, optionally, use an additional script,     <code>script/service_account.sh</code>, to create a GCP service account.  This sets     the correct roles/permissions on the account based on a list kept in the     script - this will need to be maintained.</li> <li>Terraform creates a hosting VPC and a subnetwork for the deployment, together   with the necessary firewall rules.</li> <li>Terraform creates a supporting GCS bucket. This bucket is not only used   during deployment, but also provides a long-term storage for clusters   operating within this deployment.</li> <li>Terraform sets up a Pub/Sub topic for communication between the service   machine and clusters.</li> <li>Terraform provisions a compute engine virtual machine to be the service   machine. A startup script is then executed on the service machine to set up   the software environment for HPC Toolkit and Django, and start the web and   application servers.</li> </ul>"},{"location":"developers/developer_guide/#access-the-service-machine","title":"Access the service machine","text":"<p>By default, access to the service machine is restricted to authorised users (the owner/editor of the hosting GCP project or other users delegated with sufficient permissions). Use one of the following two methods to access the system after a new deployment:</p> <ul> <li>SSH into the service machine directly from the GCP console of the hosting GCP   project.</li> <li>Edit the hosting VM instance by uploading the public SSH key of a client   machine to grant SSH access.</li> </ul> <p>Immediately after login, run <code>sudo su -l gcluster</code> to become the gcluster user. This user account was created during the deployment to be the owner of the FrontEnd files.</p>"},{"location":"developers/developer_guide/#directory-structures-on-service-machine","title":"Directory structures on service machine","text":"<p>The home directory of the gcluster account is at <code>/opt/gcluster</code>. For a new deployment, the following four sub-directories are created:</p> <ul> <li><code>go</code> - the development environment of the Go programming language, required to build Google HPC Toolkit</li> <li><code>hpc-toolkit</code> - a clone of the Google HPC Toolkit project. The <code>ghpc</code> binary    should have already been built during the deployment. The <code>frontend</code>    sub-directory contains the Django-based web application for the FrontEnd and    other supporting files.</li> <li><code>django-env</code> - a Python 3 virtual environment containing everything required    to support Django development. To activate this environment:    <code>source ~/django-env/bin/activate</code>. Doing so may be required during the    development, e.g., when running the Django <code>manage.py</code> script for    administration tasks.</li> <li><code>run</code> -\u00a0 directory for run-time data, including the following log files:</li> <li><code>nginx-access.log</code> - web server access log.</li> <li><code>nginx-error.log</code> - web server error log.</li> <li><code>supvisor.log</code> -\u00a0 Django application server log. Python <code>print</code> from      Django source files will appear in this file for debugging purposes.</li> <li><code>django.log</code> - additional debugging information generated by the Python      logging module is writen here.</li> </ul>"},{"location":"developers/developer_guide/#run-time-data","title":"Run-time data","text":""},{"location":"developers/developer_guide/#for-cloud-resources","title":"For cloud resources","text":"<p>Run-time data to support creating and managing cloud resources are generated and stored in the following sub-directories within <code>hpc-toolkit/frontend</code> on the service machine:</p> <ul> <li><code>clusters/cluster_\\&lt;id&gt;</code> - holding run-time data for a cluster. <code>\\&lt;id&gt;</code> here has a one-to-one mapping to the IDs shown in the frontend's cluster list page. The directory contains the following:</li> <li><code>cluster.yaml</code> - input file for <code>ghpc</code>, generated based on information     collected from the web interface.</li> <li><code>\\&lt;cluster_name&gt;_\\&lt;random_id&gt;/primary</code> - Terraform files generated by    <code>ghpc</code> to create the cluster, and log files from running    <code>terraform init/validate/plan/apply</code>. Should there be a need to manually     clean up the associated cloud resources, run <code>terraform destroy</code> here.</li> <li><code>vpcs/vpc_\\&lt;id&gt;</code> - similar to above but holding run-time data for a virtual   network. Currently creating custom mode VPC is not yet supported by HPC   Toolkit. A custom set of Terraform configurations are used.</li> <li><code>fs/fs_\\&lt;id&gt;</code> - similar to above but holding run-time data for a filesystem.   Currently only GCP Filestore is supported.</li> </ul> <p>Note that life cycles of VPCs and Filestores are managed independently to those of clusters.</p>"},{"location":"developers/developer_guide/#for-applications","title":"For applications","text":"<p>Application data is stored in the shared filesystem <code>/opt/cluster</code>. It contains the following sub-directories:</p> <ul> <li><code>/opt/cluster/spack</code> contains a Spack v0.17.1 installation.</li> <li>When applications are installed via the web interface, supporting files are   saved in <code>/opt/cluster/install/&lt;application_id&gt;</code> where <code>&lt;application_id&gt;</code> can   be located from the web interface.</li> <li>For a Spack installation, a job script <code>install.sh</code> is generated to     submit a Slurm job to the selected partition to run <code>spack install</code> of     the desired package.</li> <li>For a custom installation, a job script <code>install_submit.sh</code> is generated     to submit a Slurm job to the selected partition to execute <code>job.sh</code> which     contains the custom installation steps.</li> <li>After each successful installation, Spack application binaries are stored at   <code>/opt/cluster/spack/opt/spack/linux-centos7-&lt;arch&gt;</code> where <code>&lt;arch&gt;</code> is the   architecture of the processors on which the binaries get built, such as   <code>cascadelake</code> or <code>zen2</code>.</li> <li>Standard output and error files for Slurm jobs are stored in the working   directories and also uploaded to the GCS bucket associated with the   deployment at <code>gs://&lt;deployment_name&gt;-&lt;deployment_zone&gt;-storage/clusters/&lt;cluster_id&gt;/installs/&lt;application_id&gt;/stdout|err</code>   so that they remain available even if the clusters are destroyed.</li> </ul>"},{"location":"developers/developer_guide/#for-jobs","title":"For jobs","text":"<p>Job data is stored in the shared filesystem <code>/home/&lt;username&gt;</code> for each user. Here <code>&lt;username&gt;</code> is the OS Login username, which is generated by Google and will be different from the user's normal UNIX name. The home directories contain the following:</p> <ul> <li>When a job is submitted from the web interface, supporting files are saved in   <code>/home/&lt;username&gt;/jobs/&lt;job_id&gt;</code> where <code>&lt;job_id&gt;</code> can be located from the web   interface.</li> <li>When running a Spack application, a job script <code>submit.sh</code> is generated to   submit a Slurm job. This script performs a <code>spack load</code> to set up the   application environment and then invoke <code>job.sh</code> which contains the   user-supplied custom commands to run the job.</li> <li>Standard output and error files for Slurm jobs are uploaded to the GCS bucket   associated with the deployment at the following URLs:   <code>gs://&lt;deployment_name&gt;-&lt;deployment_zone&gt;-storage/clusters/&lt;cluster_id&gt;/jobs/&lt;job_id&gt;/stdout|err</code>.</li> </ul> <p>Note that a special home directory is created at <code>/home/root_jobs</code> to host jobs submitted by the Django superusers. For convenience they do not need Google identities and their jobs are run as root on the clusters.</p>"},{"location":"developers/developer_guide/#django-development","title":"Django development","text":""},{"location":"developers/developer_guide/#database-design","title":"Database Design","text":"<p>Django is great at building data-driven applications. The major system components, such as clusters, applications, and jobs, can easily map to Django data models. The database design of this system is best shown with a UML diagram. This was generated using a function available in the Python django-extensions package (depending on the Python pydotplus package and graphviz package to create the image output). To generate the UML diagram, run from the command line:</p> <pre><code>python manage.py graph_models -a -X &lt;classes_to_exclude&gt; -o UML_output.png\n</code></pre> <p>To simplify the output and exclude Django's internal models, append a list of comma-separated class names after the -X flag. The result is shown below:</p> <p></p> <p>Note that the CloudResource model is at the base of all cloud resources including network components, storage components, compute instance (representing a single VM), clusters, and Workbenches.</p>"},{"location":"developers/developer_guide/#code-layout","title":"Code Layout","text":"<p>The top few layers of the directory hierarchy of the HPC Toolkit FrontEnd define the major components:</p> dir description <code>hpc-toolkit/frontend/</code> Top level <code>.../cli/</code> client commandline interface <code>.../docs/</code> documentation <code>.../infrastructure_files/</code> Support files for deploying cloud infrastructure <code>.../tf/</code> Terraform files for deploying the HPC FrontEnd <code>.../website/</code> Source code for the HPC Frontend website"},{"location":"developers/developer_guide/#infrastructure-files","title":"Infrastructure Files","text":"dir description <code>.../frontend/infrastructure_files/</code> Top level <code>.../cluster_startup/</code> Bootstrap startup scripts <code>.../gcs_bucket/</code> Common configuration files (scripts, Ansible) to store in GCS <code>.../vpc_tf/</code> Terraform templates for creating VPCs <code>.../workbench_tf/</code> Terraform templates for creating Workbenches <p>These directories hold all the support infrastructure files which are used to create, provision, and initialize the cloud resources which may be created via the HPC Toolkit FrontEnd.  The VPC Terraform and Workbench Terraform files may eventually migrate into HPC Toolkit YAML files.</p> <p>The files under <code>gcs_bucket</code> contain the more in-depth startup scripts and configuration information for the FrontEnd webserver as well as for new clusters.  During the initial deployment of the HPC Toolkit FrontEnd, this directory is copied to a new Google Cloud Storage bucket which is then used for storing these startup codes as well as additional cluster information, such as log files.  When clusters are created in Google Cloud, the initial bootstrap template startup script (from the <code>cluster_startup</code> directory) are set to be the instance startup script.  These scripts are responsible for downloading from Google Cloud Storage the Ansible repository which is stored in the <code>gcs_bucket/clusters/ansible_setup/</code>, and running Ansible to initialize the instance.</p> <p>The source-code for the cluster client-side Command &amp; Control daemon is stored here as well, under <code>.../ansible_setup/roles/c2_daemon/files/ghpcfe_c2daemon.py</code></p>"},{"location":"developers/developer_guide/#website","title":"Website","text":"dir description <code>.../frontend/website/</code> Top level <code>.../ghpcfe/</code> Frontend Application dir <code>....../cluster_manager/</code> Utilities for cloud &amp; backend operations <code>....../management/</code> Extra Django setup commands <code>....../migrations/</code> Database migration scripts <code>....../static/</code> Images, Javascript and other static web collateral <code>....../templates/</code> Web view templates <code>....../views/</code> Python files for model views <code>.../templates/</code> All-Social plugin Templates <code>.../website/</code> Django core website configuration (including <code>settings.py</code>) <code>.../manage.py</code> Core Django application management script <p>As with many Django-based web applications, the HPC Toolkit FrontEnd Django application is broken across multiple directories, each responsible for some critical subcomponent of the overall application, implementing the MVT (model, view, template) architecture.  The <code>ghpcfe/</code> directory hosts the pieces specific to the HPC Toolkit FrontEnd, whereas the other directories are more Django-focused.</p> <p>Under <code>ghpcfe/</code>, there are a variety of directories as show in the above table.  Directly inside this directory is the majority of the Python files which make up the Frontend web application. Of particular interest would be <code>models.py</code>, which stores the DB models and <code>forms.py</code>, for custom web forms. There are a sufficiently large number of Django views in the application, so the prototypical <code>views.py</code> is broken into its own Python package.</p> <p>Also under <code>ghpcfe/</code> is the <code>cluster_manager</code> directory, which contains most of the \"backend\" code responsible for gathering cloud information as well as creating, controlling, and configuring cloud resources.  Of particular interest here are the files:</p> <ul> <li><code>c2.py</code>: Responsible for bidirectional communication between the FrontEnd and   any created clusters.</li> <li><code>cloud_info.py</code>: Provides many utilities for querying information from Google   Cloud about instance types, pricing, VPCs and so forth</li> <li><code>cluster_info.py</code>: Responsible for creating clusters and keeping track of   local-to-frontend cluster metadata</li> </ul> <p>Note that these \"backend\" functions are often invoked asynchronously if the tasks performed are time-consuming. Support to asynchronous views were introduced in Django v3.1.</p> <p>Finally, <code>templates</code> directory contains the web view templates. Django ships with its own template engine to process these template files and insert dynamic contents to them.</p>"},{"location":"developers/developer_guide/#workbenches-architecture","title":"Workbenches Architecture","text":"<p>The workbench process is fairly straight-forward. Gather configuration values from the FrontEnd and pass them to Terraform to control the creation of the workbench instance. This is done directly via Terraform as the HPC Toolkit does not currently support Vertex AI Workbenches.</p>"},{"location":"developers/developer_guide/#infrastructure-files_1","title":"Infrastructure files","text":"<p>Workbenches are created using a template configuration in <code>hpc-toolkit/frontend/infrastructure_files/workbench_tf</code>. The Terraform template was originally based on the Terraform template provided by the Google Cloud Platform Rad-Lab git repo however the configuration diverged during early development. The main reason for this divergence was to accommodate running the Jupyter notebook as a specific OSLogin user rather than the generic Jupyter user which would make it impossible to interact properly with any mounted shared storage.</p> <p>The process of creating the workbench files is mostly contained within the file <code>hpc-toolkit/frontend/website/ghpcfe/cluster_manager/workbenchinfo.py</code>. The <code>copy_terraform()</code> routine copies files from the <code>infrastructure_files</code> directory while the <code>prepare_terraform_vars()</code> routine creates a <code>terraform.tfvars</code> file within the <code>hpc-toolkit/frontend/workbenches/workbench_##</code> directory to provide the following info gathered by the FrontEnd during the workbench creation process:</p> <ul> <li>region</li> <li>zone</li> <li>project_name</li> <li>subnet_name</li> <li>machine_type</li> <li>boot_disk_type</li> <li>boot_disk_size_gb</li> <li>trusted_users</li> <li>image_family</li> <li>owner_id</li> <li>wb_startup_script_name</li> <li>wb_startup_script_bucket</li> </ul>"},{"location":"developers/developer_guide/#storage-mount-points","title":"Storage mount points","text":"<p>Storage mount points are configured on the second part of the creation process. This is done via a Django UpdateView form at <code>https://$FRONTEND.URL/workbench/update/##</code> with the main configuration fields disabled as Terraform does not support modification of an existing Vertex AI workbench, the workbench would be destroyed and recreated.</p> <p>Additionally the mount points are added to the startup script and there is no method in the frontend to re-run this startup script to mount any additional mount points therefore the UpdateView form is only presented during the creation process. Once information on the mount points is collected the startup script can be generated.</p>"},{"location":"developers/developer_guide/#startup-script","title":"Startup script","text":"<p>The startup script is generated by the <code>copy_startup_script()</code> process in <code>cluster_manager/workbenchinfo.py</code>. This process has two parts. The first part is generated using information gathered by the frontend and passes the user's social ID number set by the owner_id field. It also passes any configured mount points into the startup script before the second part of the startup script is copied from <code>infrastructure_files/gcs_bucket/workbench/startup_script_template.sh</code>.</p> <p>The startup script runs the following processes when the workbench instance boots:</p> <ul> <li>Query instance metadata for list of users and filter based on the users   social ID number to discover the correct format of their OSLogin username.</li> <li>Install nfs-common package via <code>apt-get</code>.</li> <li>Make temporary jupyterhome directory in /tmp/ and set user ownership.</li> <li>Make home directory for the user and set user ownership.</li> <li>Copy Jupyter configuration files from <code>/home/jupyter/.jupyter</code> to   <code>/tmp/jupyterhome/.jupyter</code>.</li> <li>Create <code>DATA_LOSS_WARNING.txt</code> file with warning message.</li> <li>Configure specified mount points in order specified on FrontEnd.</li> <li>Add symlink to <code>/tmp/jupyterhome/</code> which will serve as working directory on   the web interface.</li> <li>This process of mounting and symlinking means the mountpoint will appear     in both the jupyter notebook web interface working directory and in the     expected location in the root filesystem.</li> <li>Append mount points to <code>DATA_LOSS_WARNING.txt</code> file.</li> <li>If <code>/home</code> was not mounted as a mount point then create a symlink to   <code>/home</code> in <code>/tmp/jupyterhome</code>.</li> <li>Modify Jupyter config to reflect username and new working directory.</li> <li>Update <code>/lib/systemd/system/jupyter.service</code> systemd service file to reflect   username and new working directory.</li> <li>Run <code>systemctl</code> daemon-reload and restart Jupyter service.</li> <li>Without updating the Jupyter config and restarting the service then the     Jupyter notebook would be running as the jupyter user. This would break     permissions used on any mounted shared storage.</li> </ul>"},{"location":"users/user_guide/","title":"User Guide","text":"<p>This guide is for standard users of the Open Front End, who can access HPC clusters and applications that have been setup by administrators. Users can prepare, submit and run jobs on the cluster through the Front End.</p>"},{"location":"users/user_guide/#access-to-the-system","title":"Access to the System","text":"<p>An administrator should have arranged access to the system for a standard user:</p> <ul> <li>A URL should be provided on which an instance of the HPC Toolkit Front End is deployed.</li> <li>The Google identity of the user should be whitelisted to access the instance.</li> <li>The user should be set as an authorised users on existing HPC clusters.</li> <li>Admins can optionally set up quotas to restrict the amount of resources that can be consumed by individual users.</li> </ul> <p>Discuss requirements with the admin user in your organisation:</p> <ul> <li>Applications may require certain instance types to run efficiently on GCP. Admins may be able to create new Slurm partitions to support new instance types.</li> <li>Some applications may require additional configurations at install time e.g. to switch on optional features. Admins can provide binaries for additional application versions/variants.</li> </ul> <p>On first visit, click the Login link on the home page, then click the Login with Google link. The system will then attempt the authenticate your account through OAuth with Google.</p>"},{"location":"users/user_guide/#clusters","title":"Clusters","text":"<p>Shown on the cluster page is a list of active clusters the current user is authorised to use. Clusters are created and managed by admin users so messages on these pages are for information only. The underlying infrastructures, such as network and storage components, are also managed by admin users so those pages are not accessible by ordinary users.</p>"},{"location":"users/user_guide/#applications","title":"Applications","text":"<p>Shown on the application page is a list of pre-installed applications on authorised clusters. Applications are set up by admin users so messages on these pages are for information only. Application detail pages provide extra information regarding the packages.</p> <p>There are three types of applications:</p> <ul> <li>Those installed via the Spack package   manager.</li> <li>Those installed from custom scripts as prepared by the admin users.</li> <li>Those manually installed on the clusters by admin users and then registered   with this system.</li> </ul> <p>Most open-source applications are covered by Spack. For in-house applications, or those not yet supported by Spack, custom installation is often a good option. Alternatively, admin users can manually install applications on a shared filesystem and register them to the system by providing installation paths and load commands (e.g. modulefiles). In practice, end users do not need to distinguish these different types as they have access to a unified web interface to automate job creation and submission.</p>"},{"location":"users/user_guide/#jobs","title":"Jobs","text":"<p>From an application page, click the New Job action to set up a job.</p> <p>In most cases, users do not need to be concerned about the application's software environment because the system handles that automatically. For example, if an application has been set up as a Spack package, the system will automatically invoke <code>spack load</code> to configure its environment, e.g. putting the application binaries in <code>$PATH</code>.</p> <p>On the other hand, users need to provide the exact steps setting up jobs through scripts. A run script can either be located at a URL or provided inline in the job creation form. A run script may provide additional steps that download and/or prepare input files before invoking the application binary. It may also perform post-processing tasks as required.</p>"},{"location":"users/user_guide/#using-cloud-storage","title":"Using Cloud Storage","text":"<p>When submitting a new job, the end user may optionally specify:</p> <ul> <li>a URL from which input files are downloaded - <code>http://</code> or <code>https://</code> URL   for an external storage, or a <code>gs://</code> URL a Google cloud storage bucket.</li> <li>a <code>gs://</code> URL to which output files are uploaded.</li> </ul> <p>The system supports using Google Cloud Storage (GCS) buckets as external storage. Here the GCS bucket is a personal one belonging to the end user (for admin users this is not to be confused with the GCS bucket that supports the deployment of this system). A one-time set-up for the GCS bucket is required per cluster: from the Actions menu of a cluster, click Authenticate to Google Cloud Storage and then follow Google's instructions to complete the authentication.</p>"},{"location":"users/user_guide/#building-logic-directly-in-run-script","title":"Building Logic Directly in Run Script","text":"<p>Users can prepare the input data directly within the run script by using arbitrary scripting. Simple tasks may be performed in this way, e.g. downloading a public dataset from GitHub, or copying files already on the shared filesystem to the working directory.</p>"},{"location":"users/user_guide/#preparing-data-manually","title":"Preparing Data Manually","text":"<p>It is also possible to prepare job data manually on the cluster. Users can always SSH into cluster login node and run arbitrary commands to prepare data in their home directories. Then job run scripts can access these files as needed.</p> <p>Manually preparing data is probably the most tedious. However, it can be most cost-effective if a large amount of data is to be transmitted over the internet, or the data is to be shared by multiple job runs. Moving large datasets from inside the run script can add significant cloud spending as all compute nodes reserved for jobs are being charged.</p>"},{"location":"users/user_guide/#benchmarks","title":"Benchmarks","text":"<p>A benchmark is effectively a collection of jobs using the same application on the same dataset. The application version and input dataset should always be the same; the compiler/libraries to build the application and the cloud instance types to run the application can differ, so that the most performant or cost-efficient ways of running the jobs can be established through benchmarks.</p> <p>New benchmarks can be created by an admin user from the Benchmarks section of the website.</p> <p>For standard users, when running a job, there is an option to associated that job to an existing benchmark, as shown in the following figure:</p> <p></p> <p>For benchmark jobs, the job script should contain logic to produce a key performance indicator (KPI) which will be sent back to the service machine and stored in the database. Job script should extract appropriate information from the job output, and use suitable scripting to create a file called <code>kpi.json</code> and place it in the current working directory. This JSON file should be in the following format:</p> <pre><code>{\n  \"result_unit\": \"some unit\",\n  \"result_value\": \"some value\"\n}\n</code></pre>"},{"location":"users/user_guide/#vertex-ai-workbenches","title":"Vertex AI Workbenches","text":"<p>Vertex AI Workbenches give the user a dedicated interactive environment to perform pre- or post-processing of data directly from the cluster, as Jupyter Notebooks.</p> <p>Please refer to the Workbench User Guide.</p>"},{"location":"users/workbench_user_guide/","title":"Workbench User Guide","text":"<p>Vertex AI Workbenches give the user a dedicated interactive environment to perform pre- or post- processing of data from the cluster. Using the FrontEnd one can create Vertex AI workbenches dedicated to the selected user. Filestore filesystems and storage from the cluster headnode can be mounted within the Workbench and accessed as the selected user.</p>"},{"location":"users/workbench_user_guide/#create-a-workbench","title":"Create a Workbench","text":"<p>From the workbench menu select the option <code>Add workbench</code>. You will then be asked to select the appropriate cloud credential. Normal users and admin users are able to create workbenches however viewer users will need to request a workbench is created by their administrator.</p> <p></p> <p>On the above page you will be asked to select name, subnet, cloud zone, Trusted User, Machine Type, Boot disk type, Boot Disk Capacity and image family.</p> <ul> <li>Name: The given name acts as a human readable id on the Front End.</li> <li>Subnet: The Subnet field determines which network the Workbench will be   located in, the available subnets will be populated by the networks added to   the Front End under the networks menu.</li> <li>Cloud zone: The cloud zone field will be populated once the subnet is   selected and will govern which zone within the region the workbench will   located in.</li> <li>Trusted User: The trusted user field sets which user owns and has access to   the workbench. This is the user that will be used within the workbench to   access any mounted shared file storage.</li> <li>Machine Type: As a normal user you will be presented with a selection of   pre-approved machine types. If you require a machine type not on this list   you will need to request an administrator adds this machine type to the   presets or the admin user will have access to create workbenches outside of   these pre-approved machine types.</li> <li>Boot disk type: The type of disk storage used for the workbench boot disk.</li> <li>Boot disk capacity: The amount of disk storage used for the workbench boot   disk.</li> <li>Image family: Currently the Open Front End supports Python3, Tensorflow,   PyTorch and R base images.</li> </ul>"},{"location":"users/workbench_user_guide/#add-storage","title":"Add Storage","text":"<p>The second part of the configuration is to add any desired shared file storage. Once the initial configuration is saved an additional configuration section will be displayed showing the options to mount any shared file storage known about by the Open Front End.</p> <p></p> <p>From this menu you will select the file storage you wish to mount, the order in which to mount and the mount point within the filesystem. It is important to remember that these filesystems can only be mounted if they are operational at the time the workbench is started.</p> <p>Within the workbench there are methods for accessing GCP cloud storage (via the <code>gsutil</code> command) and <code>git</code>.</p>"},{"location":"users/workbench_user_guide/#access-workbench","title":"Access Workbench","text":"<p>Once the workbench is is configured you will be presented with the details page, which will contain a create link to start the workbench.</p> <p></p> <p>Once the create button is pressed the Front End will initiate the creation of the new workbench. It will take a few minutes for the workbench to be available, during which time the status will show Workbench is being created:</p> <p></p> <p>Once ready an <code>Open JupyterLab</code> link will be displayed.</p> <p></p> <p>Click the <code>Open JupyterLab</code> button, the notebook will be shown in a separate page. This is exactly the same view as notebooks opened from GCP console.</p> <p></p>"},{"location":"users/workbench_user_guide/#destroy-workbench","title":"Destroy Workbench","text":"<p>If the workbench is no longer required it can be deleted via the destroy link on the workbench page.</p> <p></p> <p>It is important to remember that all data stored on the workbench instance will be deleted unless it has been saved in another place such as a shared filesystem or transferred elsewhere in another way. Once the destory button is clicked a confirmation page will be displayed.</p> <p></p> <p>You will then be returned to the workbench detail page with the status updated to \"terminating\".</p> <p></p> <p>Once the workbench is destroyed the workbench can either be deleted permanently or recreated if it is required again.</p> <p></p>"}]}